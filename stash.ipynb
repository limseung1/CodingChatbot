{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Code Splitter by LLama Index\n",
    "from llama_index.core.node_parser import CodeSplitter\n",
    "from llama_index.readers.file import FlatReader\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "documents = []\n",
    "\n",
    "for code_file in code_files:\n",
    "    raw_documents = FlatReader().load_data(Path(code_file))\n",
    "    splitter = CodeSplitter(\n",
    "        language=\"python\",\n",
    "        chunk_lines=3,  # lines per chunk\n",
    "        chunk_lines_overlap=1,  # lines overlap between chunks\n",
    "        max_chars=1500,  # max chars per chunk\n",
    "    )\n",
    "    nodes = splitter.get_nodes_from_documents(raw_documents)\n",
    "\n",
    "    for node in nodes:\n",
    "        print(node.text)\n",
    "        print(\"=======================================\")\n",
    "        document = Document(\n",
    "            page_content=node.text,\n",
    "            metadata={\n",
    "                \"filename\": node.metadata['filename'],\n",
    "            },\n",
    "        )\n",
    "        documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using OllamaLLM\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "\n",
    "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "B_SYS, E_SYS = \"<<SYSÂ»>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "system_prompt = \"\"\"You are a helpful assistant, you will use the provided context to answer user questions.\n",
    "Read the given context before answering questions and think step by step. If you can not answer a user question based on the provided context, inform the user. Do not use any other information for answering user\"\"\"\n",
    "    \n",
    "instruction = \"\"\"\n",
    "Context: {context}\n",
    "User: {question}\"\"\"\n",
    "\n",
    "def prompt_format(instruction=instruction, system_prompt=system_prompt):\n",
    "    SYSTEM_PROMPT = B_SYS + system_prompt + E_SYS\n",
    "    prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "    return prompt_template\n",
    "\n",
    "template = prompt_format()\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(model=\"codellama:13b\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), temperature=0)\n",
    "\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=chat_model, memory_key=\"chat_history\", return_messages=True\n",
    ")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    model,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "result = qa({'question': \"how do I add user?\", 'chat_history': []})\n",
    "# result['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Commiunity Ollama\n",
    "\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "\n",
    "chat_model = Ollama(\n",
    "                base_url=\"http://localhost:11434\",\n",
    "                model=\"codellama:13b\",\n",
    "                callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "            )\n",
    "\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=chat_model, memory_key=\"chat_history\", return_messages=True\n",
    ")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    chat_model,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "result = qa({'question': \"how do I add user?\", 'chat_history': []})\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
