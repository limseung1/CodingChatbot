{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./test_repo/corporate.py', './test_repo/user_management.py', './test_repo/user_utility.py']\n"
     ]
    }
   ],
   "source": [
    "# Loader\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = './test_repo'\n",
    "code_files = glob.glob(os.path.join(path, '**/*.py'), recursive=True)\n",
    "code_files = [f for f in code_files if 'non-utf8-encoding.py' not in f]\n",
    "\n",
    "print(code_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seungwonlim/Desktop/CodingChatbot/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/seungwonlim/Desktop/CodingChatbot/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/seungwonlim/Desktop/CodingChatbot/.venv/lib/python3.9/site-packages/tree_sitter/__init__.py:36: FutureWarning: Language(path, name) is deprecated. Use Language(ptr, name) instead.\n",
      "  warn(\"{} is deprecated. Use {} instead.\".format(old, new), FutureWarning)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (581 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Parser\n",
    "import os\n",
    "from constants import Language\n",
    "from treesitter.treesitter import Treesitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "programming_language = \"python\"\n",
    "chunk_size = 30\n",
    "chunk_overlap = 0\n",
    "documents = []\n",
    "tokenizer = AutoTokenizer.from_pretrained('hkunlp/instructor-large')\n",
    "\n",
    "for code_file in code_files:\n",
    "    with open(code_file, \"r\", encoding=\"utf-8\") as file:\n",
    "        file_bytes = file.read().encode()\n",
    "\n",
    "        treesitter_parser = Treesitter.create_treesitter(Language.PYTHON)\n",
    "        treesitterNodes = treesitter_parser.parse(file_bytes)\n",
    "\n",
    "        for node in treesitterNodes:\n",
    "            method_source_code = node.method_source_code\n",
    "            filename = os.path.basename(code_file)\n",
    "\n",
    "            # # Character as unit\n",
    "            # code_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "            #     language=programming_language,\n",
    "            #     chunk_size=chunk_size,\n",
    "            #     chunk_overlap=chunk_overlap,\n",
    "            # )\n",
    "\n",
    "           # # Tokens as unit\n",
    "            code_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "                tokenizer,\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "            )\n",
    "            \n",
    "            splitted_documents = code_splitter.split_text(method_source_code)\n",
    "            for splitted_document in splitted_documents:\n",
    "\n",
    "                # length = len(tokenizer(splitted_document))\n",
    "                # print(length)\n",
    "                # print(node.name)\n",
    "                # print (\"-------------------\")\n",
    "                document = Document(\n",
    "                    page_content=splitted_document,\n",
    "                    metadata={\n",
    "                        \"filename\": filename,\n",
    "                        \"method_name\": node.name,\n",
    "                    },\n",
    "                )\n",
    "                documents.append(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gr/fvlhl6y107n7g055txmpnmdm0000gn/T/ipykernel_50497/990181052.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"hkunlp/instructor-xl\")\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Embedding\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"hkunlp/instructor-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index: FAISS\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.from_documents(documents, embeddings)\n",
    "vector_retriever = db.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "keyword_retriever = BM25Retriever.from_documents(documents)\n",
    "keyword_retriever.k = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[vector_retriever, keyword_retriever], weights=[0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'filename': 'user_utility.py', 'method_name': 'blocked_for_gs'}, page_content='GS IP addresses or logged-in GS users.\\n    \"\"\"'),\n",
       " Document(metadata={'filename': 'user_utility.py', 'method_name': 'blocked_for_gs'}, page_content='if current_ip_address_is_gs() or current_user.is_gs:'),\n",
       " Document(metadata={'filename': 'user_utility.py', 'method_name': 'current_ip_address_is_gs'}, page_content='\"\"\"Return whether the current request is coming from a GS IP address.\"\"\"'),\n",
       " Document(metadata={'filename': 'user_utility.py', 'method_name': 'current_ip_address_is_gs'}, page_content='def current_ip_address_is_gs():'),\n",
       " Document(metadata={'filename': 'user_utility.py', 'method_name': 'current_ip_address_is_gs'}, page_content='for gs_address in GS_ADDRESSES:')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity Search\n",
    "question = \"gs ip whitelist\"\n",
    "docs = db.similarity_search(question, k=5)\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gr/fvlhl6y107n7g055txmpnmdm0000gn/T/ipykernel_50497/1280554788.py:25: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(\n",
      "/var/folders/gr/fvlhl6y107n7g055txmpnmdm0000gn/T/ipykernel_50497/1280554788.py:35: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa({'question': \"explain what the function current_ip_address_is_gs is doing\", 'chat_history': []})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `current_ip_address_is_gs` function appears to be checking whether the IP address of the current request is from a Google Server (GS) IP address. It does this by comparing the IP address against a list of known GS IP addresses, which are stored in the `IPv4Network` object with the prefix `119.42.139.0/24`.\n",
      "\n",
      "The function returns a boolean value indicating whether the current request is coming from a GS IP address or not. If the IP address matches one of the known GS IP addresses, it will return `True`, otherwise it will return `False`.\n",
      "\n",
      "It's worth noting that this function appears to be part of a larger system for managing user accounts and access control. The comment suggests that the function is used to re-activate a user's corp account if they are coming from a GS IP address, which may indicate that it is being used in some kind of authentication or authorization process."
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "      \"\"\"\n",
    "      <s> [INST] You are an expert programmer for \n",
    "      question-answering tasks. Use the following pieces of retrieved\n",
    "      context to answer the question, and use relevant code snippet to help answer where possible. \n",
    "      If you don't know the answer, just say that you don't know. \n",
    "      For abbreviated name, do not guess what the abbreviated name is using external knowledge. Just state as it is.\n",
    "      Use three sentences maximum and keep the answer concise.[/INST] </s> \n",
    "      [INST] Question: {question} \n",
    "      Context: {context} \n",
    "      Answer: [/INST]\n",
    "      \"\"\"\n",
    "      )\n",
    "\n",
    "chat_model = ChatOllama(model=\"codellama:13b\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), temperature=0)\n",
    "\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=chat_model, memory_key=\"chat_history\", return_messages=True\n",
    ")\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    chat_model,\n",
    "    retriever=ensemble_retriever\n",
    ")\n",
    "\n",
    "\n",
    "result = qa({'question': \"explain what the function current_ip_address_is_gs is doing\", 'chat_history': []})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "# from langchain_community.llms import Ollama\n",
    "# from langchain.chains import ConversationalRetrievalChain\n",
    "# from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "\n",
    "# chat_model = Ollama(\n",
    "#                 base_url=\"http://localhost:11434\",\n",
    "#                 model=\"codellama:13b\",\n",
    "#                 callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "#             )\n",
    "\n",
    "# memory = ConversationSummaryMemory(\n",
    "#     llm=chat_model, memory_key=\"chat_history\", return_messages=True\n",
    "# )\n",
    "\n",
    "# qa = ConversationalRetrievalChain.from_llm(\n",
    "#     chat_model,\n",
    "#     retriever=retriever\n",
    "# )\n",
    "\n",
    "# result = qa({'question': \"how do I add user?\", 'chat_history': []})\n",
    "# print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "\n",
    "# B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
    "# B_SYS, E_SYS = \"<<SYSÂ»>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
    "\n",
    "# system_prompt = \"\"\"You are a helpful assistant, you will use the provided context to answer user questions.\n",
    "# Read the given context before answering questions and think step by step. If you can not answer a user question based on the provided context, inform the user. Do not use any other information for answering user\"\"\"\n",
    "    \n",
    "# instruction = \"\"\"\n",
    "# Context: {context}\n",
    "# User: {question}\"\"\"\n",
    "\n",
    "# def prompt_format(instruction=instruction, system_prompt=system_prompt):\n",
    "#     SYSTEM_PROMPT = B_SYS + system_prompt + E_SYS\n",
    "#     prompt_template = B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
    "#     return prompt_template\n",
    "\n",
    "# template = prompt_format()\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# model = OllamaLLM(model=\"codellama:13b\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]), temperature=0)\n",
    "\n",
    "# memory = ConversationSummaryMemory(\n",
    "#     llm=chat_model, memory_key=\"chat_history\", return_messages=True\n",
    "# )\n",
    "\n",
    "# qa = ConversationalRetrievalChain.from_llm(\n",
    "#     model,\n",
    "#     retriever=retriever\n",
    "# )\n",
    "\n",
    "# result = qa({'question': \"how do I add user?\", 'chat_history': []})\n",
    "# # result['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
